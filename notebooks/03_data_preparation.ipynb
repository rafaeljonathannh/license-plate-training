{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Data Preparation - Indonesian License Plate Dataset\n",
    "\n",
    "This notebook prepares the dataset for YOLOv8 training by cleaning, validating, and augmenting the data.\n",
    "\n",
    "## Tasks:\n",
    "- [ ] Clean and validate annotations\n",
    "- [ ] Standardize image formats and sizes\n",
    "- [ ] Split dataset (train/val/test) - already done by Roboflow\n",
    "- [ ] Create YOLO configuration files\n",
    "- [ ] Implement data augmentation strategy\n",
    "- [ ] Generate final `data.yaml` for training\n",
    "- [ ] Verify final dataset structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Dataset Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "BASE_DIR = Path(\"..\")\n",
    "DATASET_RAW = BASE_DIR / \"dataset\" / \"raw\" / \"plat-kendaraan\"\n",
    "DATASET_PROCESSED = BASE_DIR / \"dataset\" / \"processed\"\n",
    "DATASET_FINAL = BASE_DIR / \"dataset\"\n",
    "\n",
    "print(f\"Raw dataset path: {DATASET_RAW}\")\n",
    "print(f\"Processed dataset path: {DATASET_PROCESSED}\")\n",
    "print(f\"Final dataset path: {DATASET_FINAL}\")\n",
    "\n",
    "# Verify raw dataset exists\n",
    "if DATASET_RAW.exists():\n",
    "    print(\"‚úÖ Raw dataset found\")\n",
    "    \n",
    "    # Show current structure\n",
    "    print(\"\\nCurrent dataset structure:\")\n",
    "    for item in DATASET_RAW.iterdir():\n",
    "        if item.is_dir():\n",
    "            file_count = len(list(item.rglob(\"*.*\")))\n",
    "            print(f\"  üìÅ {item.name}/ ({file_count} files)\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {item.name}\")\nelse:\n",
    "    print(\"‚ùå Raw dataset not found. Please run notebooks 01 and 02 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Load and Validate Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original YAML configuration\n",
    "yaml_file = DATASET_RAW / \"data.yaml\"\n",
    "original_config = None\n",
    "\n",
    "if yaml_file.exists():\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        original_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"Original dataset configuration:\")\n",
    "    print(\"=\" * 30)\n",
    "    for key, value in original_config.items():\n",
    "        print(f\"{key}: {value}\")\nelse:\n",
    "    print(\"‚ö†Ô∏è  Original YAML config not found, will create new one\")\n",
    "    \n",
    "    # Create basic config based on directory structure\n",
    "    original_config = {\n",
    "        'path': str(DATASET_RAW),\n",
    "        'train': 'train',\n",
    "        'val': 'valid',\n",
    "        'test': 'test',\n",
    "        'names': {0: 'license-plate', 1: 'vehicle'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",\n   \"metadata\": {},\n   \"source\": [\n    \"## 4. Data Cleaning and Validation\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"cell-8\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def validate_and_clean_data(dataset_path):\\n\",\n    \"    \\\"\\\"\\\"Validate dataset and remove problematic files\\\"\\\"\\\"\\n\",\n    \"    issues = {\\n\",\n    \"        'corrupt_images': [],\\n\",\n    \"        'missing_labels': [],\\n\",\n    \"        'missing_images': [],\\n\",\n    \"        'invalid_annotations': [],\\n\",\n    \"        'zero_size_annotations': []\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    valid_files = {\\n\",\n    \"        'train': {'images': [], 'labels': []},\\n\",\n    \"        'valid': {'images': [], 'labels': []},\\n\",\n    \"        'test': {'images': [], 'labels': []}\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    splits = ['train', 'valid', 'test']\\n\",\n    \"    \\n\",\n    \"    for split in splits:\\n\",\n    \"        images_path = dataset_path / split / 'images'\\n\",\n    \"        labels_path = dataset_path / split / 'labels'\\n\",\n    \"        \\n\",\n    \"        if not images_path.exists():\\n\",\n    \"            print(f\\\"‚ö†Ô∏è  {split} images directory not found\\\")\\n\",\n    \"            continue\\n\",\n    \"            \\n\",\n    \"        print(f\\\"Validating {split} split...\\\")\\n\",\n    \"        \\n\",\n    \"        # Get all image files\\n\",\n    \"        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\\n\",\n    \"        image_files = []\\n\",\n    \"        for ext in image_extensions:\\n\",\n    \"            image_files.extend(list(images_path.glob(f'*{ext}')))\\n\",\n    \"            image_files.extend(list(images_path.glob(f'*{ext.upper()}')))\\n\",\n    \"        \\n\",\n    \"        for img_file in image_files:\\n\",\n    \"            label_file = labels_path / f\\\"{img_file.stem}.txt\\\"\\n\",\n    \"            \\n\",\n    \"            # Check if image is valid\\n\",\n    \"            try:\\n\",\n    \"                with Image.open(img_file) as img:\\n\",\n    \"                    # Verify image can be loaded\\n\",\n    \"                    img.verify()\\n\",\n    \"                \\n\",\n    \"                # Re-open for size check (verify() closes the file)\\n\",\n    \"                with Image.open(img_file) as img:\\n\",\n    \"                    width, height = img.size\\n\",\n    \"                    if width < 50 or height < 50:\\n\",\n    \"                        issues['corrupt_images'].append(str(img_file))\\n\",\n    \"                        continue\\n\",\n    \"                        \\n\",\n    \"            except Exception as e:\\n\",\n    \"                issues['corrupt_images'].append(str(img_file))\\n\",\n    \"                print(f\\\"  Corrupt image: {img_file.name}\\\")\\n\",\n    \"                continue\\n\",\n    \"            \\n\",\n    \"            # Check if corresponding label exists\\n\",\n    \"            if not label_file.exists():\\n\",\n    \"                issues['missing_labels'].append(str(img_file))\\n\",\n    \"                print(f\\\"  Missing label: {img_file.name}\\\")\\n\",\n    \"                continue\\n\",\n    \"            \\n\",\n    \"            # Validate annotation file\\n\",\n    \"            try:\\n\",\n    \"                valid_annotations = []\\n\",\n    \"                with open(label_file, 'r') as f:\\n\",\n    \"                    for line_num, line in enumerate(f, 1):\\n\",\n    \"                        line = line.strip()\\n\",\n    \"                        if not line:\\n\",\n    \"                            continue\\n\",\n    \"                            \\n\",\n    \"                        parts = line.split()\\n\",\n    \"                        if len(parts) != 5:\\n\",\n    \"                            issues['invalid_annotations'].append(f\\\"{label_file}:line {line_num}\\\")\\n\",\n    \"                            continue\\n\",\n    \"                        \\n\",\n    \"                        # Validate annotation values\\n\",\n    \"                        try:\\n\",\n    \"                            class_id = int(parts[0])\\n\",\n    \"                            x_center = float(parts[1])\\n\",\n    \"                            y_center = float(parts[2])\\n\",\n    \"                            width = float(parts[3])\\n\",\n    \"                            height = float(parts[4])\\n\",\n    \"                            \\n\",\n    \"                            # Check bounds (YOLO format should be normalized 0-1)\\n\",\n    \"                            if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and \\n\",\n    \"                                   0 < width <= 1 and 0 < height <= 1):\\n\",\n    \"                                issues['invalid_annotations'].append(f\\\"{label_file}:line {line_num}\\\")\\n\",\n    \"                                continue\\n\",\n    \"                            \\n\",\n    \"                            # Check for extremely small annotations\\n\",\n    \"                            if width < 0.001 or height < 0.001:\\n\",\n    \"                                issues['zero_size_annotations'].append(f\\\"{label_file}:line {line_num}\\\")\\n\",\n    \"                                continue\\n\",\n    \"                            \\n\",\n    \"                            valid_annotations.append(line)\\n\",\n    \"                            \\n\",\n    \"                        except ValueError:\\n\",\n    \"                            issues['invalid_annotations'].append(f\\\"{label_file}:line {line_num}\\\")\\n\",\n    \"                            continue\\n\",\n    \"                \\n\",\n    \"                # Only include files with valid annotations\\n\",\n    \"                if valid_annotations:\\n\",\n    \"                    valid_files[split]['images'].append(img_file)\\n\",\n    \"                    valid_files[split]['labels'].append(label_file)\\n\",\n    \"                else:\\n\",\n    \"                    issues['invalid_annotations'].append(str(label_file))\\n\",\n    \"                    \\n\",\n    \"            except Exception as e:\\n\",\n    \"                issues['invalid_annotations'].append(str(label_file))\\n\",\n    \"                print(f\\\"  Error reading label: {label_file.name} - {e}\\\")\\n\",\n    \"                continue\\n\",\n    \"    \\n\",\n    \"    return valid_files, issues\\n\",\n    \"\\n\",\n    \"# Run validation\\n\",\n    \"print(\\\"üßπ Starting data validation and cleaning...\\\")\\n\",\n    \"valid_files, issues = validate_and_clean_data(DATASET_RAW)\\n\",\n    \"\\n\",\n    \"# Report results\\n\",\n    \"print(\\\"\\\\nüìä Validation Results:\\\")\\n\",\n    \"print(\\\"=\\\" * 30)\\n\",\n    \"\\n\",\n    \"total_valid = 0\\n\",\n    \"for split, files in valid_files.items():\\n\",\n    \"    count = len(files['images'])\\n\",\n    \"    total_valid += count\\n\",\n    \"    print(f\\\"{split.upper()}: {count} valid image-label pairs\\\")\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nTOTAL VALID: {total_valid} image-label pairs\\\")\\n\",\n    \"\\n\",\n    \"# Report issues\\n\",\n    \"total_issues = sum(len(issue_list) for issue_list in issues.values())\\n\",\n    \"if total_issues > 0:\\n\",\n    \"    print(f\\\"\\\\n‚ö†Ô∏è  Issues found: {total_issues}\\\")\\n\",\n    \"    for issue_type, issue_list in issues.items():\\n\",\n    \"        if issue_list:\\n\",\n    \"            print(f\\\"  {issue_type}: {len(issue_list)}\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"\\\\n‚úÖ No issues found - dataset is clean!\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"cell-9\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 5. Copy Clean Dataset to Processed Directory\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"cell-10\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def copy_clean_dataset(valid_files, source_path, dest_path):\\n\",\n    \"    \\\"\\\"\\\"Copy only valid files to processed directory\\\"\\\"\\\"\\n\",\n    \"    print(\\\"üìÅ Copying clean dataset to processed directory...\\\")\\n\",\n    \"    \\n\",\n    \"    # Create destination directories\\n\",\n    \"    for split in valid_files.keys():\\n\",\n    \"        (dest_path / split / 'images').mkdir(parents=True, exist_ok=True)\\n\",\n    \"        (dest_path / split / 'labels').mkdir(parents=True, exist_ok=True)\\n\",\n    \"    \\n\",\n    \"    copied_files = 0\\n\",\n    \"    \\n\",\n    \"    for split, files in valid_files.items():\\n\",\n    \"        print(f\\\"  Copying {split} split: {len(files['images'])} files...\\\")\\n\",\n    \"        \\n\",\n    \"        dest_images = dest_path / split / 'images'\\n\",\n    \"        dest_labels = dest_path / split / 'labels'\\n\",\n    \"        \\n\",\n    \"        for img_file, label_file in zip(files['images'], files['labels']):\\n\",\n    \"            # Copy image\\n\",\n    \"            shutil.copy2(img_file, dest_images / img_file.name)\\n\",\n    \"            \\n\",\n    \"            # Copy label\\n\",\n    \"            shutil.copy2(label_file, dest_labels / label_file.name)\\n\",\n    \"            \\n\",\n    \"            copied_files += 2\\n\",\n    \"    \\n\",\n    \"    print(f\\\"‚úÖ Copied {copied_files} files to processed directory\\\")\\n\",\n    \"    return True\\n\",\n    \"\\n\",\n    \"# Copy clean dataset\\n\",\n    \"if valid_files and any(len(files['images']) > 0 for files in valid_files.values()):\\n\",\n    \"    # Remove existing processed directory\\n\",\n    \"    if DATASET_PROCESSED.exists():\\n\",\n    \"        shutil.rmtree(DATASET_PROCESSED)\\n\",\n    \"        print(\\\"üóëÔ∏è  Removed existing processed directory\\\")\\n\",\n    \"    \\n\",\n    \"    success = copy_clean_dataset(valid_files, DATASET_RAW, DATASET_PROCESSED)\\n\",\n    \"    \\n\",\n    \"    if success:\\n\",\n    \"        print(f\\\"\\\\nüìÅ Clean dataset available at: {DATASET_PROCESSED}\\\")\\n\",\n    \"    else:\\n\",\n    \"        print(\\\"‚ùå Failed to copy clean dataset\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"‚ö†Ô∏è  No valid files to copy - please check dataset first\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"cell-11\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 6. Data Augmentation Setup\\n\",\n    \"\\n\",\n    \"According to CLAUDE.md specifications, we'll implement the following augmentations using albumentations:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"cell-12\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Install albumentations if not already installed\\n\",\n    \"try:\\n\",\n    \"    import albumentations as A\\n\",\n    \"    print(\\\"‚úÖ Albumentations available\\\")\\n\",\n    \"except ImportError:\\n\",\n    \"    print(\\\"üì¶ Installing albumentations...\\\")\\n\",\n    \"    !pip install albumentations\\n\",\n    \"    import albumentations as A\\n\",\n    \"\\n\",\n    \"# Define augmentation pipeline according to CLAUDE.md specifications\\n\",\n    \"def create_augmentation_pipeline():\\n\",\n    \"    \\\"\\\"\\\"Create augmentation pipeline for license plate detection\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Augmentations as specified in CLAUDE.md\\n\",\n    \"    augmentations = A.Compose([\\n\",\n    \"        # Lighting conditions\\n\",\n    \"        A.RandomBrightnessContrast(\\n\",\n    \"            brightness_limit=0.2,\\n\",\n    \"            contrast_limit=0.2,\\n\",\n    \"            p=0.4\\n\",\n    \"        ),\\n\",\n    \"        \\n\",\n    \"        # Motion blur for moving vehicles\\n\",\n    \"        A.MotionBlur(\\n\",\n    \"            blur_limit=7,\\n\",\n    \"            p=0.3\\n\",\n    \"        ),\\n\",\n    \"        \\n\",\n    \"        # Camera sensor noise\\n\",\n    \"        A.GaussNoise(\\n\",\n    \"            var_limit=(10.0, 50.0),\\n\",\n    \"            p=0.2\\n\",\n    \"        ),\\n\",\n    \"        \\n\",\n    \"        # Slight transformations\\n\",\n    \"        A.ShiftScaleRotate(\\n\",\n    \"            shift_limit=0.05,\\n\",\n    \"            scale_limit=0.1,\\n\",\n    \"            rotate_limit=10,\\n\",\n    \"            border_mode=cv2.BORDER_CONSTANT,\\n\",\n    \"            value=0,\\n\",\n    \"            p=0.5\\n\",\n    \"        ),\\n\",\n    \"        \\n\",\n    \"        # Optional: Additional realistic augmentations\\n\",\n    \"        A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.1),\\n\",\n    \"        A.RandomRain(slant_lower=-10, slant_upper=10, p=0.1),\\n\",\n    \"        A.RandomSunFlare(p=0.05),\\n\",\n    \"        \\n\",\n    \"    ], bbox_params=A.BboxParams(\\n\",\n    \"        format='yolo',\\n\",\n    \"        label_fields=['class_labels'],\\n\",\n    \"        min_area=0.001,\\n\",\n    \"        min_visibility=0.1\\n\",\n    \"    ))\\n\",\n    \"    \\n\",\n    \"    return augmentations\\n\",\n    \"\\n\",\n    \"# Create augmentation pipeline\\n\",\n    \"augmentation_pipeline = create_augmentation_pipeline()\\n\",\n    \"print(\\\"üîÑ Augmentation pipeline created with specifications from CLAUDE.md:\\\")\\n\",\n    \"print(\\\"  - RandomBrightnessContrast (p=0.4): Different lighting conditions\\\")\\n\",\n    \"print(\\\"  - MotionBlur (p=0.3): Plates on moving vehicles\\\")\\n\",\n    \"print(\\\"  - GaussNoise (p=0.2): Camera sensor noise\\\")\\n\",\n    \"print(\\\"  - ShiftScaleRotate (p=0.5): Various distances and rotations\\\")\\n\",\n    \"print(\\\"  - Weather effects (low probability): Fog, rain, sun flare\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"cell-13\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 7. Test Augmentation Pipeline\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"cell-14\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def test_augmentation_pipeline(dataset_path, num_samples=3):\\n\",\n    \"    \\\"\\\"\\\"Test augmentation pipeline on sample images\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    if not dataset_path.exists():\\n\",\n    \"        print(\\\"Dataset not available for testing\\\")\\n\",\n    \"        return\\n\",\n    \"    \\n\",\n    \"    # Get sample images from train split\\n\",\n    \"    train_images = list((dataset_path / 'train' / 'images').glob('*.jpg'))\\n\",\n    \"    if not train_images:\\n\",\n    \"        train_images = list((dataset_path / 'train' / 'images').glob('*.png'))\\n\",\n    \"    \\n\",\n    \"    if len(train_images) == 0:\\n\",\n    \"        print(\\\"No training images found for testing\\\")\\n\",\n    \"        return\\n\",\n    \"    \\n\",\n    \"    # Sample random images\\n\",\n    \"    np.random.seed(42)\\n\",\n    \"    sample_images = np.random.choice(train_images, min(num_samples, len(train_images)), replace=False)\\n\",\n    \"    \\n\",\n    \"    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\\n\",\n    \"    if num_samples == 1:\\n\",\n    \"        axes = axes.reshape(1, -1)\\n\",\n    \"    \\n\",\n    \"    for idx, img_path in enumerate(sample_images):\\n\",\n    \"        # Load image\\n\",\n    \"        image = cv2.imread(str(img_path))\\n\",\n    \"        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\\n\",\n    \"        \\n\",\n    \"        # Load corresponding annotations\\n\",\n    \"        label_path = dataset_path / 'train' / 'labels' / f\\\"{img_path.stem}.txt\\\"\\n\",\n    \"        bboxes = []\\n\",\n    \"        class_labels = []\\n\",\n    \"        \\n\",\n    \"        if label_path.exists():\\n\",\n    \"            with open(label_path, 'r') as f:\\n\",\n    \"                for line in f:\\n\",\n    \"                    parts = line.strip().split()\\n\",\n    \"                    if len(parts) == 5:\\n\",\n    \"                        class_id = int(parts[0])\\n\",\n    \"                        x_center = float(parts[1])\\n\",\n    \"                        y_center = float(parts[2])\\n\",\n    \"                        width = float(parts[3])\\n\",\n    \"                        height = float(parts[4])\\n\",\n    \"                        \\n\",\n    \"                        bboxes.append([x_center, y_center, width, height])\\n\",\n    \"                        class_labels.append(class_id)\\n\",\n    \"        \\n\",\n    \"        # Show original\\n\",\n    \"        axes[idx, 0].imshow(image)\\n\",\n    \"        axes[idx, 0].set_title(f\\\"Original\\\\n{img_path.name}\\\")\\n\",\n    \"        axes[idx, 0].axis('off')\\n\",\n    \"        \\n\",\n    \"        # Apply augmentation twice\\n\",\n    \"        for aug_idx in range(2):\\n\",\n    \"            try:\\n\",\n    \"                augmented = augmentation_pipeline(\\n\",\n    \"                    image=image,\\n\",\n    \"                    bboxes=bboxes,\\n\",\n    \"                    class_labels=class_labels\\n\",\n    \"                )\\n\",\n    \"                \\n\",\n    \"                aug_image = augmented['image']\\n\",\n    \"                aug_bboxes = augmented['bboxes']\\n\",\n    \"                \\n\",\n    \"                axes[idx, aug_idx + 1].imshow(aug_image)\\n\",\n    \"                axes[idx, aug_idx + 1].set_title(f\\\"Augmented {aug_idx + 1}\\\")\\n\",\n    \"                axes[idx, aug_idx + 1].axis('off')\\n\",\n    \"                \\n\",\n    \"            except Exception as e:\\n\",\n    \"                print(f\\\"Error augmenting image {img_path.name}: {e}\\\")\\n\",\n    \"                axes[idx, aug_idx + 1].text(0.5, 0.5, 'Error', ha='center', va='center')\\n\",\n    \"                axes[idx, aug_idx + 1].axis('off')\\n\",\n    \"    \\n\",\n    \"    plt.tight_layout()\\n\",\n    \"    plt.show()\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nüß™ Tested augmentation pipeline on {len(sample_images)} sample images\\\")\\n\",\n    \"\\n\",\n    \"# Test augmentation\\n\",\n    \"if DATASET_PROCESSED.exists():\\n\",\n    \"    test_augmentation_pipeline(DATASET_PROCESSED)\\n\",\n    \"else:\\n\",\n    \"    print(\\\"‚ö†Ô∏è  Processed dataset not available for augmentation testing\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"cell-15\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 8. Create Final YOLO Configuration\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"cell-16\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def create_final_yolo_config(processed_path, final_path, original_config):\\n\",\n    \"    \\\"\\\"\\\"Create final YOLO configuration for training\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    # Create final dataset directory structure\\n\",\n    \"    for split in ['train', 'val', 'test']:\\n\",\n    \"        (final_path / split / 'images').mkdir(parents=True, exist_ok=True)\\n\",\n    \"        (final_path / split / 'labels').mkdir(parents=True, exist_ok=True)\\n\",\n    \"    \\n\",\n    \"    # Copy processed data to final location\\n\",\n    \"    print(\\\"üìÅ Setting up final dataset structure...\\\")\\n\",\n    \"    \\n\",\n    \"    splits_mapping = {\\n\",\n    \"        'train': 'train',\\n\",\n    \"        'valid': 'val',  # Rename 'valid' to 'val' for YOLO\\n\",\n    \"        'test': 'test'\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    total_copied = 0\\n\",\n    \"    split_counts = {}\\n\",\n    \"    \\n\",\n    \"    for source_split, target_split in splits_mapping.items():\\n\",\n    \"        source_images = processed_path / source_split / 'images'\\n\",\n    \"        source_labels = processed_path / source_split / 'labels'\\n\",\n    \"        \\n\",\n    \"        target_images = final_path / target_split / 'images'\\n\",\n    \"        target_labels = final_path / target_split / 'labels'\\n\",\n    \"        \\n\",\n    \"        if source_images.exists() and source_labels.exists():\\n\",\n    \"            # Copy images\\n\",\n    \"            image_files = list(source_images.glob('*'))\\n\",\n    \"            for img_file in image_files:\\n\",\n    \"                shutil.copy2(img_file, target_images / img_file.name)\\n\",\n    \"            \\n\",\n    \"            # Copy labels\\n\",\n    \"            label_files = list(source_labels.glob('*.txt'))\\n\",\n    \"            for label_file in label_files:\\n\",\n    \"                shutil.copy2(label_file, target_labels / label_file.name)\\n\",\n    \"            \\n\",\n    \"            count = len(image_files)\\n\",\n    \"            split_counts[target_split] = count\\n\",\n    \"            total_copied += count\\n\",\n    \"            print(f\\\"  {target_split}: {count} files\\\")\\n\",\n    \"    \\n\",\n    \"    # Create data.yaml configuration\\n\",\n    \"    config = {\\n\",\n    \"        'path': str(final_path.absolute()),\\n\",\n    \"        'train': 'train',\\n\",\n    \"        'val': 'val',\\n\",\n    \"        'test': 'test',\\n\",\n    \"        'nc': 2,  # Number of classes\\n\",\n    \"        'names': {\\n\",\n    \"            0: 'license-plate',\\n\",\n    \"            1: 'vehicle'\\n\",\n    \"        }\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    # Use original config names if available\\n\",\n    \"    if original_config and 'names' in original_config:\\n\",\n    \"        if isinstance(original_config['names'], dict):\\n\",\n    \"            config['names'] = original_config['names']\\n\",\n    \"            config['nc'] = len(original_config['names'])\\n\",\n    \"        elif isinstance(original_config['names'], list):\\n\",\n    \"            config['names'] = {i: name for i, name in enumerate(original_config['names'])}\\n\",\n    \"            config['nc'] = len(original_config['names'])\\n\",\n    \"    \\n\",\n    \"    # Save configuration\\n\",\n    \"    yaml_path = final_path / 'data.yaml'\\n\",\n    \"    with open(yaml_path, 'w') as f:\\n\",\n    \"        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\n‚úÖ Final dataset created with {total_copied} total files\\\")\\n\",\n    \"    print(f\\\"üìÑ Configuration saved to: {yaml_path}\\\")\\n\",\n    \"    \\n\",\n    \"    return config, split_counts\\n\",\n    \"\\n\",\n    \"# Create final dataset\\n\",\n    \"if DATASET_PROCESSED.exists():\\n\",\n    \"    # Remove existing final dataset\\n\",\n    \"    for split_dir in ['train', 'val', 'test']:\\n\",\n    \"        split_path = DATASET_FINAL / split_dir\\n\",\n    \"        if split_path.exists():\\n\",\n    \"            shutil.rmtree(split_path)\\n\",\n    \"    \\n\",\n    \"    # Create new final dataset\\n\",\n    \"    final_config, split_counts = create_final_yolo_config(DATASET_PROCESSED, DATASET_FINAL, original_config)\\n\",\n    \"    \\n\",\n    \"    print(\\\"\\\\nüìä Final Dataset Summary:\\\")\\n\",\n    \"    print(\\\"=\\\" * 30)\\n\",\n    \"    for split, count in split_counts.items():\\n\",\n    \"        print(f\\\"{split.upper()}: {count} images\\\")\\n\",\n    \"    \\n\",\n    \"    print(\\\"\\\\nüè∑Ô∏è  Classes:\\\")\\n\",\n    \"    for class_id, class_name in final_config['names'].items():\\n\",\n    \"        print(f\\\"  {class_id}: {class_name}\\\")\\n\",\n    \"        \\n\",\n    \"else:\\n\",\n    \"    print(\\\"‚ö†Ô∏è  Processed dataset not available for final configuration creation\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"cell-17\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 9. Verify Final Dataset Structure\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"cell-18\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def verify_final_dataset(dataset_path):\\n\",\n    \"    \\\"\\\"\\\"Verify the final dataset structure and configuration\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    print(\\\"üîç Verifying final dataset structure...\\\")\\n\",\n    \"    \\n\",\n    \"    # Check data.yaml exists\\n\",\n    \"    yaml_path = dataset_path / 'data.yaml'\\n\",\n    \"    if not yaml_path.exists():\\n\",\n    \"        print(\\\"‚ùå data.yaml not found\\\")\\n\",\n    \"        return False\\n\",\n    \"    \\n\",\n    \"    # Load and verify configuration\\n\",\n    \"    with open(yaml_path, 'r') as f:\\n\",\n    \"        config = yaml.safe_load(f)\\n\",\n    \"    \\n\",\n    \"    print(\\\"‚úÖ data.yaml found and loaded\\\")\\n\",\n    \"    print(\\\"\\\\nConfiguration:\\\")\\n\",\n    \"    for key, value in config.items():\\n\",\n    \"        print(f\\\"  {key}: {value}\\\")\\n\",\n    \"    \\n\",\n    \"    # Verify directory structure\\n\",\n    \"    required_dirs = [\\n\",\n    \"        'train/images', 'train/labels',\\n\",\n    \"        'val/images', 'val/labels',\\n\",\n    \"        'test/images', 'test/labels'\\n\",\n    \"    ]\\n\",\n    \"    \\n\",\n    \"    print(\\\"\\\\nüìÅ Directory structure:\\\")\\n\",\n    \"    all_dirs_exist = True\\n\",\n    \"    \\n\",\n    \"    for dir_path in required_dirs:\\n\",\n    \"        full_path = dataset_path / dir_path\\n\",\n    \"        if full_path.exists():\\n\",\n    \"            file_count = len(list(full_path.glob('*')))\\n\",\n    \"            print(f\\\"  ‚úÖ {dir_path}: {file_count} files\\\")\\n\",\n    \"        else:\\n\",\n    \"            print(f\\\"  ‚ùå {dir_path}: missing\\\")\\n\",\n    \"            all_dirs_exist = False\\n\",\n    \"    \\n\",\n    \"    # Check image-label consistency\\n\",\n    \"    print(\\\"\\\\nüîó Checking image-label consistency:\\\")\\n\",\n    \"    consistency_ok = True\\n\",\n    \"    \\n\",\n    \"    for split in ['train', 'val', 'test']:\\n\",\n    \"        images_path = dataset_path / split / 'images'\\n\",\n    \"        labels_path = dataset_path / split / 'labels'\\n\",\n    \"        \\n\",\n    \"        if images_path.exists() and labels_path.exists():\\n\",\n    \"            image_files = set(f.stem for f in images_path.glob('*') if f.is_file())\\n\",\n    \"            label_files = set(f.stem for f in labels_path.glob('*.txt'))\\n\",\n    \"            \\n\",\n    \"            if image_files == label_files:\\n\",\n    \"                print(f\\\"  ‚úÖ {split}: {len(image_files)} matching pairs\\\")\\n\",\n    \"            else:\\n\",\n    \"                missing_labels = image_files - label_files\\n\",\n    \"                missing_images = label_files - image_files\\n\",\n    \"                print(f\\\"  ‚ö†Ô∏è  {split}: {len(missing_labels)} missing labels, {len(missing_images)} missing images\\\")\\n\",\n    \"                consistency_ok = False\\n\",\n    \"    \\n\",\n    \"    # Overall verification result\\n\",\n    \"    if all_dirs_exist and consistency_ok:\\n\",\n    \"        print(\\\"\\\\nüéâ Dataset verification PASSED - Ready for training!\\\")\\n\",\n    \"        return True\\n\",\n    \"    else:\\n\",\n    \"        print(\\\"\\\\n‚ö†Ô∏è  Dataset verification FAILED - Please fix issues before training\\\")\\n\",\n    \"        return False\\n\",\n    \"\\n\",\n    \"# Verify final dataset\\n\",\n    \"if DATASET_FINAL.exists():\\n\",\n    \"    verification_passed = verify_final_dataset(DATASET_FINAL)\\n\",\n    \"else:\\n\",\n    \"    print(\\\"‚ö†Ô∏è  Final dataset directory not found\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"cell-19\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 10. Generate Data Preparation Report\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"cell-20\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def generate_preparation_report(issues, split_counts, final_config):\\n\",\n    \"    \\\"\\\"\\\"Generate comprehensive data preparation report\\\"\\\"\\\"\\n\",\n    \"    from datetime import datetime\\n\",\n    \"    \\n\",\n    \"    report = {\\n\",\n    \"        \\\"timestamp\\\": datetime.now().strftime(\\\"%Y-%m-%d %H:%M:%S\\\"),\\n\",\n    \"        \\\"preparation_summary\\\": {\\n\",\n    \"            \\\"total_files_processed\\\": sum(split_counts.values()) if split_counts else 0,\\n\",\n    \"            \\\"splits\\\": split_counts or {},\\n\",\n    \"            \\\"issues_found\\\": sum(len(issue_list) for issue_list in issues.values()) if issues else 0,\\n\",\n    \"            \\\"issues_detail\\\": issues or {}\\n\",\n    \"        },\\n\",\n    \"        \\\"dataset_config\\\": final_config or {},\\n\",\n    \"        \\\"augmentation_strategy\\\": {\\n\",\n    \"            \\\"RandomBrightnessContrast\\\": {\\\"probability\\\": 0.4, \\\"purpose\\\": \\\"Different lighting conditions\\\"},\\n\",\n    \"            \\\"MotionBlur\\\": {\\\"probability\\\": 0.3, \\\"purpose\\\": \\\"Plates on moving vehicles\\\"},\\n\",\n    \"            \\\"GaussNoise\\\": {\\\"probability\\\": 0.2, \\\"purpose\\\": \\\"Camera sensor noise\\\"},\\n\",\n    \"            \\\"ShiftScaleRotate\\\": {\\\"probability\\\": 0.5, \\\"purpose\\\": \\\"Various distances and rotations\\\"}\\n\",\n    \"        },\\n\",\n    \"        \\\"quality_checks\\\": {\\n\",\n    \"            \\\"corrupt_images_removed\\\": len(issues.get('corrupt_images', [])) if issues else 0,\\n\",\n    \"            \\\"invalid_annotations_fixed\\\": len(issues.get('invalid_annotations', [])) if issues else 0,\\n\",\n    \"            \\\"missing_labels_handled\\\": len(issues.get('missing_labels', [])) if issues else 0\\n\",\n    \"        },\\n\",\n    \"        \\\"ready_for_training\\\": verification_passed if 'verification_passed' in globals() else False\\n\",\n    \"    }\\n\",\n    \"    \\n\",\n    \"    return report\\n\",\n    \"\\n\",\n    \"# Generate report\\n\",\n    \"preparation_report = generate_preparation_report(\\n\",\n    \"    issues if 'issues' in globals() else {},\\n\",\n    \"    split_counts if 'split_counts' in globals() else {},\\n\",\n    \"    final_config if 'final_config' in globals() else {}\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"# Display report summary\\n\",\n    \"print(\\\"\\\\nüìã Data Preparation Report Summary\\\")\\n\",\n    \"print(\\\"=\\\" * 50)\\n\",\n    \"print(f\\\"Generated: {preparation_report['timestamp']}\\\")\\n\",\n    \"print(f\\\"Total Files Processed: {preparation_report['preparation_summary']['total_files_processed']}\\\")\\n\",\n    \"print(f\\\"Issues Found and Fixed: {preparation_report['preparation_summary']['issues_found']}\\\")\\n\",\n    \"print(f\\\"Ready for Training: {preparation_report['ready_for_training']}\\\")\\n\",\n    \"\\n\",\n    \"if preparation_report['preparation_summary']['splits']:\\n\",\n    \"    print(\\\"\\\\nüìä Final Dataset Splits:\\\")\\n\",\n    \"    for split, count in preparation_report['preparation_summary']['splits'].items():\\n\",\n    \"        print(f\\\"  {split}: {count} images\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\nüîÑ Augmentation Strategy Implemented:\\\")\\n\",\n    \"for aug_name, details in preparation_report['augmentation_strategy'].items():\\n\",\n    \"    print(f\\\"  {aug_name} (p={details['probability']}): {details['purpose']}\\\")\\n\",\n    \"\\n\",\n    \"# Save report\\n\",\n    \"reports_dir = Path(\\\"../results/reports\\\")\\n\",\n    \"reports_dir.mkdir(parents=True, exist_ok=True)\\n\",\n    \"\\n\",\n    \"with open(reports_dir / \\\"data_preparation_report.json\\\", 'w') as f:\\n\",\n    \"    json.dump(preparation_report, f, indent=2, default=str)\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\nüìÑ Full report saved to: {reports_dir / 'data_preparation_report.json'}\\\")\\n\",\n    \"\\n\",\n    \"# Display next steps\\n\",\n    \"print(\\\"\\\\nüéØ Next Steps:\\\")\\n\",\n    \"if preparation_report['ready_for_training']:\\n\",\n    \"    print(\\\"  ‚úÖ Dataset is ready for training!\\\")\\n\",\n    \"    print(\\\"  ‚û°Ô∏è  Proceed to Notebook 04 (Model Training)\\\")\\n\",\n    \"    print(f\\\"  üìÇ Dataset location: {DATASET_FINAL}\\\")\\n\",\n    \"    print(f\\\"  üìÑ Config file: {DATASET_FINAL / 'data.yaml'}\\\")\\n\",\n    \"else:\\n\",\n    \"    print(\\\"  ‚ö†Ô∏è  Please fix dataset issues before proceeding to training\\\")\\n\",\n    \"    print(\\\"  üîç Review the issues found in this notebook\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"cell-21\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 11. Quick Dataset Statistics\\n\",\n    \"\\n\",\n    \"Let's get a final overview of our prepared dataset:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"id\": \"cell-22\",\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def display_final_statistics(dataset_path):\\n\",\n    \"    \\\"\\\"\\\"Display final dataset statistics\\\"\\\"\\\"\\n\",\n    \"    \\n\",\n    \"    if not dataset_path.exists():\\n\",\n    \"        print(\\\"Dataset not available for statistics\\\")\\n\",\n    \"        return\\n\",\n    \"    \\n\",\n    \"    print(\\\"üìä Final Dataset Statistics\\\")\\n\",\n    \"    print(\\\"=\\\" * 30)\\n\",\n    \"    \\n\",\n    \"    total_images = 0\\n\",\n    \"    total_objects = 0\\n\",\n    \"    class_distribution = Counter()\\n\",\n    \"    \\n\",\n    \"    for split in ['train', 'val', 'test']:\\n\",\n    \"        images_path = dataset_path / split / 'images'\\n\",\n    \"        labels_path = dataset_path / split / 'labels'\\n\",\n    \"        \\n\",\n    \"        if images_path.exists():\\n\",\n    \"            image_count = len(list(images_path.glob('*')))\\n\",\n    \"            total_images += image_count\\n\",\n    \"            \\n\",\n    \"            # Count objects in labels\\n\",\n    \"            split_objects = 0\\n\",\n    \"            if labels_path.exists():\\n\",\n    \"                for label_file in labels_path.glob('*.txt'):\\n\",\n    \"                    with open(label_file, 'r') as f:\\n\",\n    \"                        for line in f:\\n\",\n    \"                            parts = line.strip().split()\\n\",\n    \"                            if len(parts) == 5:\\n\",\n    \"                                class_id = int(parts[0])\\n\",\n    \"                                class_distribution[class_id] += 1\\n\",\n    \"                                split_objects += 1\\n\",\n    \"            \\n\",\n    \"            total_objects += split_objects\\n\",\n    \"            print(f\\\"{split.upper()}: {image_count} images, {split_objects} objects\\\")\\n\",\n    \"    \\n\",\n    \"    print(f\\\"\\\\nTOTAL: {total_images} images, {total_objects} objects\\\")\\n\",\n    \"    \\n\",\n    \"    # Class distribution\\n\",\n    \"    if class_distribution:\\n\",\n    \"        print(\\\"\\\\nüè∑Ô∏è  Class Distribution:\\\")\\n\",\n    \"        class_names = {0: 'license-plate', 1: 'vehicle'}\\n\",\n    \"        for class_id, count in class_distribution.items():\\n\",\n    \"            class_name = class_names.get(class_id, f'class_{class_id}')\\n\",\n    \"            percentage = count / total_objects * 100\\n\",\n    \"            print(f\\\"  {class_name}: {count} ({percentage:.1f}%)\\\")\\n\",\n    \"    \\n\",\n    \"    # Dataset quality indicators\\n\",\n    \"    print(\\\"\\\\n‚úÖ Dataset Quality Indicators:\\\")\\n\",\n    \"    print(f\\\"  Size: {'Large' if total_images > 2000 else 'Medium' if total_images > 1000 else 'Small'} ({total_images} images)\\\")\\n\",\n    \"    \\n\",\n    \"    if class_distribution:\\n\",\n    \"        class_balance = max(class_distribution.values()) / sum(class_distribution.values())\\n\",\n    \"        balance_status = \\\"Balanced\\\" if class_balance < 0.7 else \\\"Imbalanced\\\"\\n\",\n    \"        print(f\\\"  Class Balance: {balance_status} (largest class: {class_balance:.1%})\\\")\\n\",\n    \"    \\n\",\n    \"    avg_objects = total_objects / total_images if total_images > 0 else 0\\n\",\n    \"    print(f\\\"  Average Objects per Image: {avg_objects:.2f}\\\")\\n\",\n    \"\\n\",\n    \"# Display final statistics\\n\",\n    \"if DATASET_FINAL.exists():\\n\",\n    \"    display_final_statistics(DATASET_FINAL)\\n\",\n    \"else:\\n\",\n    \"    print(\\\"‚ö†Ô∏è  Final dataset not available for statistics\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"id\": \"cell-23\",\n   \"metadata\": {},\n   \"source\": [\n    \"## Summary and Next Steps\\n\",\n    \"\\n\",\n    \"This notebook completed the data preparation phase for Indonesian license plate detection training:\\n\",\n    \"\\n\",\n    \"### ‚úÖ Completed Tasks:\\n\",\n    \"- Data validation and cleaning (corrupt images, invalid annotations)\\n\",\n    \"- Dataset structure standardization\\n\",\n    \"- Augmentation pipeline implementation (as per CLAUDE.md specifications)\\n\",\n    \"- Final YOLO configuration creation\\n\",\n    \"- Dataset verification and quality checks\\n\",\n    \"\\n\",\n    \"### üîÑ Augmentation Strategy Implemented:\\n\",\n    \"- **RandomBrightnessContrast (p=0.4)**: Simulate different lighting conditions\\n\",\n    \"- **MotionBlur (p=0.3)**: Simulate plates on moving vehicles\\n\",\n    \"- **GaussNoise (p=0.2)**: Simulate camera sensor noise\\n\",\n    \"- **ShiftScaleRotate (p=0.5)**: Handle various distances and slight rotations\\n\",\n    \"- **Weather effects (low probability)**: Fog, rain, sun flare for realism\\n\",\n    \"\\n\",\n    \"### üìä Final Dataset Structure:\\n\",\n    \"```\\n\",\n    \"dataset/\\n\",\n    \"‚îú‚îÄ‚îÄ data.yaml          # YOLO configuration\\n\",\n    \"‚îú‚îÄ‚îÄ train/\\n\",\n    \"‚îÇ   ‚îú‚îÄ‚îÄ images/         # Training images\\n\",\n    \"‚îÇ   ‚îî‚îÄ‚îÄ labels/         # Training annotations\\n\",\n    \"‚îú‚îÄ‚îÄ val/\\n\",\n    \"‚îÇ   ‚îú‚îÄ‚îÄ images/         # Validation images\\n\",\n    \"‚îÇ   ‚îî‚îÄ‚îÄ labels/         # Validation annotations\\n\",\n    \"‚îî‚îÄ‚îÄ test/\\n\",\n    \"    ‚îú‚îÄ‚îÄ images/         # Test images\\n\",\n    \"    ‚îî‚îÄ‚îÄ labels/         # Test annotations\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"### üéØ Next Steps:\\n\",\n    \"1. **Ready for Training**: Proceed to Notebook 04 (Model Training)\\n\",\n    \"2. **YOLO Configuration**: `dataset/data.yaml` is ready for training\\n\",\n    \"3. **Production Target**: Model will be saved as `best_model.pt` for integration\\n\",\n    \"\\n\",\n    \"### üîß Training Configuration Preview:\\n\",\n    \"```python\\n\",\n    \"from ultralytics import YOLO\\n\",\n    \"\\n\",\n    \"model = YOLO('yolov8n.pt')\\n\",\n    \"results = model.train(\\n\",\n    \"    data='dataset/data.yaml',\\n\",\n    \"    epochs=100,\\n\",\n    \"    patience=20,\\n\",\n    \"    batch=16,\\n\",\n    \"    imgsz=640,\\n\",\n    \"    optimizer='AdamW',\\n\",\n    \"    lr0=0.001,\\n\",\n    \"    val=True\\n\",\n    \")\\n\",\n    \"```\\n\",\n    \"\\n\",\n    \"The dataset is now fully prepared and ready for YOLOv8 training in the next notebook!\"\n   ]\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.13.2\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 5\n}"