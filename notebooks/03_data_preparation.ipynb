{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Data Preparation - Indonesian License Plate Dataset\n",
    "\n",
    "This notebook prepares the dataset for YOLOv8 training by cleaning, validating, and augmenting the data.\n",
    "\n",
    "## Tasks:\n",
    "- [ ] Clean and validate annotations\n",
    "- [ ] Standardize image formats and sizes\n",
    "- [ ] Split dataset (train/val/test) - already done by Roboflow\n",
    "- [ ] Create YOLO configuration files\n",
    "- [ ] Implement data augmentation strategy\n",
    "- [ ] Generate final `data.yaml` for training\n",
    "- [ ] Verify final dataset structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import json\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Dataset Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "BASE_DIR = Path(\"..\")\n",
    "DATASET_RAW = BASE_DIR / \"dataset\" / \"raw\" / \"plat-kendaraan\"\n",
    "DATASET_PROCESSED = BASE_DIR / \"dataset\" / \"processed\"\n",
    "DATASET_FINAL = BASE_DIR / \"dataset\"\n",
    "\n",
    "print(f\"Raw dataset path: {DATASET_RAW}\")\n",
    "print(f\"Processed dataset path: {DATASET_PROCESSED}\")\n",
    "print(f\"Final dataset path: {DATASET_FINAL}\")\n",
    "\n",
    "# Verify raw dataset exists\n",
    "if DATASET_RAW.exists():\n",
    "    print(\"‚úÖ Raw dataset found\")\n",
    "    \n",
    "    # Show current structure\n",
    "    print(\"\\nCurrent dataset structure:\")\n",
    "    for item in DATASET_RAW.iterdir():\n",
    "        if item.is_dir():\n",
    "            file_count = len(list(item.rglob(\"*.*\")))\n",
    "            print(f\"  üìÅ {item.name}/ ({file_count} files)\")\n",
    "        else:\n",
    "            print(f\"  üìÑ {item.name}\")\n",
    "else:\n",
    "    print(\"‚ùå Raw dataset not found. Please run notebooks 01 and 02 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Load and Validate Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original YAML configuration\n",
    "yaml_file = DATASET_RAW / \"data.yaml\"\n",
    "original_config = None\n",
    "\n",
    "if yaml_file.exists():\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        original_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"Original dataset configuration:\")\n",
    "    print(\"=\" * 30)\n",
    "    for key, value in original_config.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Original YAML config not found, will create new one\")\n",
    "    \n",
    "    # Create basic config based on directory structure\n",
    "    original_config = {\n",
    "        'path': str(DATASET_RAW),\n",
    "        'train': 'train',\n",
    "        'val': 'valid',\n",
    "        'test': 'test',\n",
    "        'names': {0: 'license-plate', 1: 'vehicle'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_clean_data(dataset_path):\n",
    "    \"\"\"Validate dataset and remove problematic files\"\"\"\n",
    "    issues = {\n",
    "        'corrupt_images': [],\n",
    "        'missing_labels': [],\n",
    "        'missing_images': [],\n",
    "        'invalid_annotations': [],\n",
    "        'zero_size_annotations': []\n",
    "    }\n",
    "    \n",
    "    valid_files = {\n",
    "        'train': {'images': [], 'labels': []},\n",
    "        'valid': {'images': [], 'labels': []},\n",
    "        'test': {'images': [], 'labels': []}\n",
    "    }\n",
    "    \n",
    "    splits = ['train', 'valid', 'test']\n",
    "    \n",
    "    for split in splits:\n",
    "        images_path = dataset_path / split / 'images'\n",
    "        labels_path = dataset_path / split / 'labels'\n",
    "        \n",
    "        if not images_path.exists():\n",
    "            print(f\"‚ö†Ô∏è  {split} images directory not found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Validating {split} split...\")\n",
    "        \n",
    "        # Get all image files\n",
    "        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "        image_files = []\n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(list(images_path.glob(f'*{ext}')))\n",
    "            image_files.extend(list(images_path.glob(f'*{ext.upper()}')))\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            label_file = labels_path / f\"{img_file.stem}.txt\"\n",
    "            \n",
    "            # Check if image is valid\n",
    "            try:\n",
    "                with Image.open(img_file) as img:\n",
    "                    # Verify image can be loaded\n",
    "                    img.verify()\n",
    "                \n",
    "                # Re-open for size check (verify() closes the file)\n",
    "                with Image.open(img_file) as img:\n",
    "                    width, height = img.size\n",
    "                    if width < 50 or height < 50:\n",
    "                        issues['corrupt_images'].append(str(img_file))\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                issues['corrupt_images'].append(str(img_file))\n",
    "                print(f\"  Corrupt image: {img_file.name}\")\n",
    "                continue\n",
    "            \n",
    "            # Check if corresponding label exists\n",
    "            if not label_file.exists():\n",
    "                issues['missing_labels'].append(str(img_file))\n",
    "                print(f\"  Missing label: {img_file.name}\")\n",
    "                continue\n",
    "            \n",
    "            # Validate annotation file\n",
    "            try:\n",
    "                valid_annotations = []\n",
    "                with open(label_file, 'r') as f:\n",
    "                    for line_num, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                            \n",
    "                        parts = line.split()\n",
    "                        if len(parts) != 5:\n",
    "                            issues['invalid_annotations'].append(f\"{label_file}:line {line_num}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Validate annotation values\n",
    "                        try:\n",
    "                            class_id = int(parts[0])\n",
    "                            x_center = float(parts[1])\n",
    "                            y_center = float(parts[2])\n",
    "                            width = float(parts[3])\n",
    "                            height = float(parts[4])\n",
    "                            \n",
    "                            # Check bounds (YOLO format should be normalized 0-1)\n",
    "                            if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and \n",
    "                                   0 < width <= 1 and 0 < height <= 1):\n",
    "                                issues['invalid_annotations'].append(f\"{label_file}:line {line_num}\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Check for extremely small annotations\n",
    "                            if width < 0.001 or height < 0.001:\n",
    "                                issues['zero_size_annotations'].append(f\"{label_file}:line {line_num}\")\n",
    "                                continue\n",
    "                            \n",
    "                            valid_annotations.append(line)\n",
    "                            \n",
    "                        except ValueError:\n",
    "                            issues['invalid_annotations'].append(f\"{label_file}:line {line_num}\")\n",
    "                            continue\n",
    "                \n",
    "                # Only include files with valid annotations\n",
    "                if valid_annotations:\n",
    "                    valid_files[split]['images'].append(img_file)\n",
    "                    valid_files[split]['labels'].append(label_file)\n",
    "                else:\n",
    "                    issues['invalid_annotations'].append(str(label_file))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                issues['invalid_annotations'].append(str(label_file))\n",
    "                print(f\"  Error reading label: {label_file.name} - {e}\")\n",
    "                continue\n",
    "    \n",
    "    return valid_files, issues\n",
    "\n",
    "# Run validation\n",
    "print(\"üßπ Starting data validation and cleaning...\")\n",
    "valid_files, issues = validate_and_clean_data(DATASET_RAW)\n",
    "\n",
    "# Report results\n",
    "print(\"\\nüìä Validation Results:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "total_valid = 0\n",
    "for split, files in valid_files.items():\n",
    "    count = len(files['images'])\n",
    "    total_valid += count\n",
    "    print(f\"{split.upper()}: {count} valid image-label pairs\")\n",
    "\n",
    "print(f\"\\nTOTAL VALID: {total_valid} image-label pairs\")\n",
    "\n",
    "# Report issues\n",
    "total_issues = sum(len(issue_list) for issue_list in issues.values())\n",
    "if total_issues > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  Issues found: {total_issues}\")\n",
    "    for issue_type, issue_list in issues.items():\n",
    "        if issue_list:\n",
    "            print(f\"  {issue_type}: {len(issue_list)}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No issues found - dataset is clean!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation Setup\n",
    "\n",
    "According to CLAUDE.md specifications, we'll implement the following augmentations using albumentations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install albumentations if not already installed\n",
    "try:\n",
    "    import albumentations as A\n",
    "    print(\"‚úÖ Albumentations available\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ Installing albumentations...\")\n",
    "    !pip install albumentations\n",
    "    import albumentations as A\n",
    "\n",
    "# Define augmentation pipeline according to CLAUDE.md specifications\n",
    "def create_augmentation_pipeline():\n",
    "    \"\"\"Create augmentation pipeline for license plate detection\"\"\"\n",
    "    \n",
    "    # Augmentations as specified in CLAUDE.md\n",
    "    augmentations = A.Compose([\n",
    "        # Lighting conditions\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.2,\n",
    "            contrast_limit=0.2,\n",
    "            p=0.4\n",
    "        ),\n",
    "        \n",
    "        # Motion blur for moving vehicles\n",
    "        A.MotionBlur(\n",
    "            blur_limit=7,\n",
    "            p=0.3\n",
    "        ),\n",
    "        \n",
    "        # Camera sensor noise\n",
    "        A.GaussNoise(\n",
    "            var_limit=(10.0, 50.0),\n",
    "            p=0.2\n",
    "        ),\n",
    "        \n",
    "        # Slight transformations\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=10,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0,\n",
    "            p=0.5\n",
    "        ),\n",
    "        \n",
    "        # Optional: Additional realistic augmentations\n",
    "        A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.1),\n",
    "        A.RandomRain(slant_lower=-10, slant_upper=10, p=0.1),\n",
    "        A.RandomSunFlare(p=0.05),\n",
    "        \n",
    "    ], bbox_params=A.BboxParams(\n",
    "        format='yolo',\n",
    "        label_fields=['class_labels'],\n",
    "        min_area=0.001,\n",
    "        min_visibility=0.1\n",
    "    ))\n",
    "    \n",
    "    return augmentations\n",
    "\n",
    "# Create augmentation pipeline\n",
    "augmentation_pipeline = create_augmentation_pipeline()\n",
    "print(\"üîÑ Augmentation pipeline created with specifications from CLAUDE.md:\")\n",
    "print(\"  - RandomBrightnessContrast (p=0.4): Different lighting conditions\")\n",
    "print(\"  - MotionBlur (p=0.3): Plates on moving vehicles\")\n",
    "print(\"  - GaussNoise (p=0.2): Camera sensor noise\")\n",
    "print(\"  - ShiftScaleRotate (p=0.5): Various distances and rotations\")\n",
    "print(\"  - Weather effects (low probability): Fog, rain, sun flare\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements data preparation for YOLOv8 training:\n",
    "\n",
    "### ‚úÖ Completed:\n",
    "- Data validation and cleaning\n",
    "- Augmentation pipeline setup (CLAUDE.md specs)\n",
    "- Dataset structure verification\n",
    "\n",
    "### üéØ Next Steps:\n",
    "Run the cells above to:\n",
    "1. Validate your dataset\n",
    "2. Set up augmentation pipeline\n",
    "3. Proceed to Notebook 04 for training\n",
    "\n",
    "The dataset should now be ready for YOLOv8 training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}