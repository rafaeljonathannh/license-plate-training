{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration - Indonesian License Plate Dataset\n",
    "\n",
    "Quick dataset overview and verification before training.\n",
    "\n",
    "## Tasks:\n",
    "- [ ] Dataset structure verification\n",
    "- [ ] Basic statistics (image count, label count)\n",
    "- [ ] Data quality checks\n",
    "- [ ] Sample visualization\n",
    "- [ ] Training readiness confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from collections import Counter\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define paths (license-plate-training as root)\n",
    "ROOT_DIR = Path(\"..\").resolve()  # From notebooks/ to license-plate-training/\n",
    "DATASET_PATH = ROOT_DIR / \"dataset\" / \"plat-kendaraan\"\n",
    "MODELS_DIR = ROOT_DIR / \"models\"\n",
    "RESULTS_DIR = ROOT_DIR / \"results\"\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Working directory: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path with updated root\n",
    "dataset_path = DATASET_PATH\n",
    "\n",
    "if dataset_path.exists():\n",
    "    print(f\"âœ… Dataset found: {dataset_path}\")\n",
    "    print(f\"ðŸ“ Full path: {dataset_path.absolute()}\")\n",
    "else:\n",
    "    print(f\"âŒ Dataset not found at: {dataset_path}\")\n",
    "    print(\"Please run notebook 01 first to download the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Structure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot analyze structure - dataset not found\n"
     ]
    }
   ],
   "source": [
    "if dataset_path.exists():\n",
    "    print(\"ðŸ“‚ Dataset Structure:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Show main directories and files\n",
    "    for item in sorted(dataset_path.iterdir()):\n",
    "        if item.is_dir():\n",
    "            # Count files in subdirectories\n",
    "            total_files = len(list(item.rglob(\"*.*\")))\n",
    "            images_count = len(list((item / \"images\").glob(\"*.*\"))) if (item / \"images\").exists() else 0\n",
    "            labels_count = len(list((item / \"labels\").glob(\"*.*\"))) if (item / \"labels\").exists() else 0\n",
    "            \n",
    "            print(f\"  ðŸ“ {item.name}/\")\n",
    "            if images_count > 0 or labels_count > 0:\n",
    "                print(f\"      ðŸ–¼ï¸  images: {images_count}\")\n",
    "                print(f\"      ðŸ·ï¸  labels: {labels_count}\")\n",
    "            else:\n",
    "                print(f\"      ðŸ“„ files: {total_files}\")\n",
    "        else:\n",
    "            print(f\"  ðŸ“„ {item.name}\")\n",
    "    \n",
    "    # Check for data.yaml\n",
    "    yaml_file = dataset_path / \"data.yaml\"\n",
    "    if yaml_file.exists():\n",
    "        print(f\"\\nâœ… Configuration file found: data.yaml\")\n",
    "        \n",
    "        try:\n",
    "            with open(yaml_file, 'r') as f:\n",
    "                config = yaml.safe_load(f)\n",
    "            \n",
    "            print(\"\\nðŸ“‹ Dataset Configuration:\")\n",
    "            for key, value in config.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not read YAML: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Configuration file not found: data.yaml\")\n",
    "else:\n",
    "    print(\"Cannot analyze structure - dataset not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Split Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot perform analysis - dataset not found\n"
     ]
    }
   ],
   "source": [
    "if dataset_path.exists():\n",
    "    splits = ['train', 'valid', 'test']\n",
    "    total_images = 0\n",
    "    total_labels = 0\n",
    "    \n",
    "    print(\"ðŸ“Š Dataset Split Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    split_stats = {}\n",
    "    \n",
    "    for split in splits:\n",
    "        split_path = dataset_path / split\n",
    "        \n",
    "        if split_path.exists():\n",
    "            images_dir = split_path / 'images'\n",
    "            labels_dir = split_path / 'labels'\n",
    "            \n",
    "            # Count files\n",
    "            image_files = []\n",
    "            if images_dir.exists():\n",
    "                image_files = list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.png')) + list(images_dir.glob('*.jpeg'))\n",
    "            \n",
    "            label_files = []\n",
    "            if labels_dir.exists():\n",
    "                label_files = list(labels_dir.glob('*.txt'))\n",
    "            \n",
    "            image_count = len(image_files)\n",
    "            label_count = len(label_files)\n",
    "            \n",
    "            split_stats[split] = {\n",
    "                'images': image_count,\n",
    "                'labels': label_count,\n",
    "                'match': image_count == label_count\n",
    "            }\n",
    "            \n",
    "            total_images += image_count\n",
    "            total_labels += label_count\n",
    "            \n",
    "            match_symbol = \"âœ…\" if image_count == label_count else \"âš ï¸\"\n",
    "            print(f\"{split.upper():>6}: {image_count:>6} images | {label_count:>6} labels | {match_symbol}\")\n",
    "        else:\n",
    "            print(f\"{split.upper():>6}: âŒ Directory not found\")\n",
    "            split_stats[split] = {'images': 0, 'labels': 0, 'match': False}\n",
    "    \n",
    "    print(\"=\" * 40)\n",
    "    overall_match = \"âœ…\" if total_images == total_labels else \"âš ï¸\"\n",
    "    print(f\"{'TOTAL':>6}: {total_images:>6} images | {total_labels:>6} labels | {overall_match}\")\n",
    "    \n",
    "    # Calculate split percentages\n",
    "    if total_images > 0:\n",
    "        print(\"\\nðŸ“Š Split Distribution:\")\n",
    "        for split, stats in split_stats.items():\n",
    "            percentage = (stats['images'] / total_images) * 100\n",
    "            print(f\"  {split.capitalize():>5}: {percentage:5.1f}%\")\n",
    "    \n",
    "    # Training readiness check\n",
    "    print(\"\\nðŸŽ¯ Training Readiness Check:\")\n",
    "    checks = {\n",
    "        \"Dataset exists\": dataset_path.exists(),\n",
    "        \"YAML config found\": (dataset_path / \"data.yaml\").exists(),\n",
    "        \"Train split exists\": split_stats.get('train', {}).get('images', 0) > 0,\n",
    "        \"Valid split exists\": split_stats.get('valid', {}).get('images', 0) > 0,\n",
    "        \"Images-labels match\": all(stats['match'] for stats in split_stats.values()),\n",
    "        \"Sufficient data (>100 images)\": total_images > 100\n",
    "    }\n",
    "    \n",
    "    all_ready = True\n",
    "    for check, passed in checks.items():\n",
    "        symbol = \"âœ…\" if passed else \"âŒ\"\n",
    "        print(f\"  {symbol} {check}\")\n",
    "        if not passed:\n",
    "            all_ready = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    if all_ready:\n",
    "        print(\"ðŸš€ READY FOR TRAINING! Proceed to notebook 04\")\n",
    "    else:\n",
    "        print(\"âš ï¸  TRAINING NOT READY - Fix issues above first\")\n",
    "    print(\"=\"*50)\n",
    "        \n",
    "else:\n",
    "    print(\"Cannot perform analysis - dataset not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a quick overview of the Indonesian license plate dataset:\n",
    "\n",
    "### âœ… Completed:\n",
    "- Dataset structure verification\n",
    "- Split analysis (train/valid/test)\n",
    "- Data quality checks\n",
    "- Training readiness confirmation\n",
    "\n",
    "### ðŸŽ¯ Next Steps:\n",
    "If all checks pass:\n",
    "1. **Ready for Training**: Proceed to `04_model_training.ipynb`\n",
    "2. **Training Parameters**: Use standard YOLO settings\n",
    "3. **Expected Performance**: Target mAP@0.5 > 0.85\n",
    "\n",
    "If issues found:\n",
    "1. **Fix dataset issues**: Re-run `01_setup_and_dataset.ipynb`\n",
    "2. **Check file integrity**: Verify all images/labels are accessible\n",
    "3. **Resolve path issues**: Ensure all paths are correct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
