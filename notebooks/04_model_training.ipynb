{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Model Training - Indonesian License Plate Detection\n",
    "\n",
    "This notebook trains a YOLOv8 model for Indonesian license plate detection using the prepared dataset.\n",
    "\n",
    "## Tasks:\n",
    "- [ ] Load pre-trained YOLOv8 model\n",
    "- [ ] Configure hyperparameters (as per CLAUDE.md specifications)\n",
    "- [ ] Setup Weights & Biases (W&B) experiment tracking\n",
    "- [ ] Start training with progress monitoring\n",
    "- [ ] Visualize training metrics (loss, mAP)\n",
    "- [ ] Save training checkpoints\n",
    "- [ ] Implement early stopping\n",
    "- [ ] Export best model for production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import YOLOv8\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import LOGGER\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Setup Paths and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path configuration\n",
    "BASE_DIR = Path(\"..\")\n",
    "DATASET_PATH = BASE_DIR / \"dataset\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "EXPERIMENTS_DIR = MODELS_DIR / \"experiments\"\n",
    "CHECKPOINTS_DIR = MODELS_DIR / \"checkpoints\"\n",
    "FINAL_MODELS_DIR = MODELS_DIR / \"final\"\n",
    "RESULTS_DIR = BASE_DIR / \"results\"\n",
    "\n",
    "# Create directories\n",
    "for directory in [EXPERIMENTS_DIR, CHECKPOINTS_DIR, FINAL_MODELS_DIR, RESULTS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Dataset path: {DATASET_PATH}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "\n",
    "# Verify dataset configuration\n",
    "data_yaml = DATASET_PATH / \"data.yaml\"\n",
    "if data_yaml.exists():\n",
    "    with open(data_yaml, 'r') as f:\n",
    "        dataset_config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"\\n✅ Dataset configuration found:\")\n",
    "    for key, value in dataset_config.items():\n",
    "        print(f\"  {key}: {value}\")\nelse:\n",
    "    print(\"\\n❌ Dataset configuration not found. Please run Notebook 03 first.\")\n",
    "    dataset_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Setup Weights & Biases (W&B) Experiment Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and setup Weights & Biases\n",
    "try:\n",
    "    import wandb\n",
    "    print(\"✅ Weights & Biases available\")\nexcept ImportError:\n",
    "    print(\"📦 Installing Weights & Biases...\")\n",
    "    !pip install wandb\n",
    "    import wandb\n",
    "\n",
    "# Initialize W&B project\n",
    "def setup_wandb_tracking():\n",
    "    \"\"\"Setup Weights & Biases experiment tracking\"\"\"\n",
    "    \n",
    "    project_name = \"indonesian-license-plate-detection\"\n",
    "    experiment_name = f\"yolov8n-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=project_name,\n",
    "        name=experiment_name,\n",
    "        config={\n",
    "            \"model_type\": \"YOLOv8n\",\n",
    "            \"dataset\": \"Indonesian License Plates\",\n",
    "            \"task\": \"object_detection\",\n",
    "            \"framework\": \"ultralytics\",\n",
    "            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        },\n",
    "        tags=[\"yolov8\", \"license-plate\", \"indonesian\", \"detection\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"🔬 W&B experiment initialized: {experiment_name}\")\n",
    "    return experiment_name\n",
    "\n",
    "# Setup tracking (optional - can be skipped if W&B not available)\n",
    "try:\n",
    "    experiment_name = setup_wandb_tracking()\n",
    "    use_wandb = True\nexcept Exception as e:\n",
    "    print(f\"⚠️  W&B setup failed: {e}\")\n",
    "    print(\"Training will continue without W&B tracking\")\n",
    "    use_wandb = False\n",
    "    experiment_name = f\"yolov8n-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Training Configuration (CLAUDE.md Specifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters as specified in CLAUDE.md\n",
    "training_config = {\n",
    "    # Basic parameters\n",
    "    \"model\": \"yolov8n.pt\",  # Pre-trained model\n",
    "    \"data\": str(data_yaml) if data_yaml.exists() else \"dataset/data.yaml\",\n",
    "    \"epochs\": 100,\n",
    "    \"patience\": 20,  # Early stopping patience\n",
    "    \"batch\": 16,     # Baseline for GPU >=8GB VRAM. Reduce to 8 if memory errors\n",
    "    \"imgsz\": 640,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"lr0\": 0.001,    # Initial learning rate\n",
    "    \"val\": True,\n",
    "    \n",
    "    # Additional parameters for better training\n",
    "    \"save\": True,\n",
    "    \"save_period\": 10,  # Save checkpoint every 10 epochs\n",
    "    \"cache\": False,     # Don't cache images (to save memory)\n",
    "    \"device\": \"0\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"workers\": 4,       # Number of dataloader workers\n",
    "    \"project\": str(EXPERIMENTS_DIR),\n",
    "    \"name\": experiment_name,\n",
    "    \"exist_ok\": True,\n",
    "    \"pretrained\": True,\n",
    "    \"verbose\": True,\n",
    "    \n",
    "    # Performance targets from CLAUDE.md\n",
    "    \"target_map50\": 0.85,  # mAP@0.5 > 0.85\n",
    "    \"confidence_threshold\": 0.3,  # >= 0.3\n",
    "}\n",
    "\n",
    "# Adjust batch size if low memory\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_memory < 8:\n",
    "        training_config[\"batch\"] = 8\n",
    "        print(f\"⚠️  Reduced batch size to 8 due to limited GPU memory ({gpu_memory:.1f} GB)\")\n",
    "    else:\n",
    "        print(f\"✅ Using batch size 16 with {gpu_memory:.1f} GB GPU memory\")\nelse:\n",
    "    training_config[\"batch\"] = 4  # Very small batch for CPU training\n",
    "    training_config[\"workers\"] = 2\n",
    "    print(\"⚠️  CPU training detected - reduced batch size to 4\")\n",
    "\n",
    "print(\"\\n🔧 Training Configuration:\")\nprint(\"=\" * 30)\nfor key, value in training_config.items():\n    if key not in [\"target_map50\", \"confidence_threshold\"]:\n        print(f\"{key}: {value}\")\n\n# Log to W&B if available\nif use_wandb:\n    wandb.config.update(training_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model\n",
    "print(\"🤖 Loading YOLOv8 model...\")\n",
    "\n",
    "try:\n",
    "    # Load pre-trained YOLOv8n model\n",
    "    model = YOLO(training_config[\"model\"])\n",
    "    print(f\"✅ Model loaded: {training_config['model']}\")\n",
    "    \n",
    "    # Display model info\n",
    "    model.info(verbose=False)\n",
    "    \n",
    "    # Check model device\n",
    "    print(f\"Model device: {model.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Dataset Verification Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset_before_training(data_path):\n",
    "    \"\"\"Final verification before starting training\"\"\"\n",
    "    \n",
    "    print(\"🔍 Final dataset verification...\")\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"❌ Dataset path not found: {data_path}\")\n",
    "        return False\n",
    "    \n",
    "    # Check data.yaml\n",
    "    yaml_file = data_path / \"data.yaml\"\n",
    "    if not yaml_file.exists():\n",
    "        print(\"❌ data.yaml not found\")\n",
    "        return False\n",
    "    \n",
    "    # Load configuration\n",
    "    with open(yaml_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Check required splits\n",
    "    required_splits = ['train', 'val']\n",
    "    for split in required_splits:\n",
    "        if split not in config:\n",
    "            print(f\"❌ Missing {split} split in configuration\")\n",
    "            return False\n",
    "        \n",
    "        # Check directories exist\n",
    "        images_dir = data_path / split / \"images\"\n",
    "        labels_dir = data_path / split / \"labels\"\n",
    "        \n",
    "        if not images_dir.exists():\n",
    "            print(f\"❌ Missing {split}/images directory\")\n",
    "            return False\n",
    "        \n",
    "        if not labels_dir.exists():\n",
    "            print(f\"❌ Missing {split}/labels directory\")\n",
    "            return False\n",
    "        \n",
    "        # Count files\n",
    "        image_count = len(list(images_dir.glob(\"*\")))  \n",
    "        label_count = len(list(labels_dir.glob(\"*.txt\")))\n",
    "        \n",
    "        print(f\"✅ {split}: {image_count} images, {label_count} labels\")\n",
    "        \n",
    "        if image_count == 0:\n",
    "            print(f\"❌ No images found in {split} split\")\n",
    "            return False\n",
    "        \n",
    "        if image_count != label_count:\n",
    "            print(f\"⚠️  Image-label mismatch in {split}: {image_count} vs {label_count}\")\n",
    "    \n",
    "    # Check classes\n",
    "    if 'nc' not in config or 'names' not in config:\n",
    "        print(\"❌ Missing class configuration\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"✅ Classes: {config['nc']} ({config['names']})\")\n",
    "    print(\"✅ Dataset verification passed\")\n",
    "    return True\n",
    "\n",
    "# Run verification\nif dataset_config:\n    dataset_ready = verify_dataset_before_training(DATASET_PATH)\nelse:\n    print(\"❌ Cannot verify dataset - configuration not loaded\")\n    dataset_ready = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training if dataset is ready\nif dataset_ready and model:\n    print(\"🚀 Starting training...\")\n    print(f\"Experiment: {experiment_name}\")\n    print(f\"Target: mAP@0.5 > {training_config['target_map50']}\")\n    \n    try:\n        # Start training with the specified configuration\n        results = model.train(**{\n            k: v for k, v in training_config.items() \n            if k not in [\"model\", \"target_map50\", \"confidence_threshold\"]\n        })\n        \n        print(\"✅ Training completed successfully\")\n        \n        # Save training results\n        training_results = {\n            \"experiment_name\": experiment_name,\n            \"training_config\": training_config,\n            \"final_results\": str(results) if results else \"No results\",\n            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n        \n        # Save to file\n        results_file = RESULTS_DIR / \"metrics\" / f\"{experiment_name}_results.json\"\n        results_file.parent.mkdir(parents=True, exist_ok=True)\n        \n        with open(results_file, 'w') as f:\n            json.dump(training_results, f, indent=2, default=str)\n        \n        print(f\"📊 Training results saved to: {results_file}\")\n        \n    except Exception as e:\n        print(f\"❌ Training failed: {e}\")\n        raise\n        \nelse:\n    print(\"❌ Cannot start training - dataset not ready or model not loaded\")\n    print(\"Please run previous cells to fix issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. Analyze Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_results(experiment_dir):\n",
    "    \"\"\"Analyze and visualize training results\"\"\"\n",
    "    \n",
    "    results_csv = experiment_dir / \"results.csv\"\n",
    "    \n",
    "    if not results_csv.exists():\n",
    "        print(f\"❌ Results file not found: {results_csv}\")\n",
    "        return None\n",
    "    \n",
    "    # Load training results\n",
    "    df = pd.read_csv(results_csv)\n",
    "    df.columns = df.columns.str.strip()  # Remove whitespace\n",
    "    \n",
    "    print(f\"📊 Training completed in {len(df)} epochs\")\n",
    "    \n",
    "    # Get final metrics\n",
    "    final_metrics = df.iloc[-1]\n",
    "    \n",
    "    print(\"\\n🎯 Final Performance Metrics:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Key metrics\n",
    "    metrics_to_show = {\n",
    "        'metrics/mAP50(B)': 'mAP@0.5',\n",
    "        'metrics/mAP50-95(B)': 'mAP@0.5:0.95',\n",
    "        'metrics/precision(B)': 'Precision',\n",
    "        'metrics/recall(B)': 'Recall',\n",
    "        'val/box_loss': 'Box Loss',\n",
    "        'val/cls_loss': 'Class Loss',\n",
    "        'val/dfl_loss': 'DFL Loss'\n",
    "    }\n",
    "    \n",
    "    for col, name in metrics_to_show.items():\n",
    "        if col in df.columns:\n",
    "            value = final_metrics[col]\n",
    "            print(f\"{name}: {value:.4f}\")\n",
    "    \n",
    "    # Check if target achieved\n",
    "    map50_col = 'metrics/mAP50(B)'\n",
    "    if map50_col in df.columns:\n",
    "        final_map50 = final_metrics[map50_col]\n",
    "        target_map50 = training_config['target_map50']\n",
    "        \n",
    "        if final_map50 >= target_map50:\n",
    "            print(f\"\\n🎉 TARGET ACHIEVED: mAP@0.5 = {final_map50:.4f} (target: {target_map50})\")\n",
    "        else:\n",
    "            print(f\"\\n⚠️  Target not reached: mAP@0.5 = {final_map50:.4f} (target: {target_map50})\")\n",
    "            print(\"Consider: longer training, data augmentation, or hyperparameter tuning\")\n",
    "    \n",
    "    # Create training plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Training and validation losses\n",
    "    if 'train/box_loss' in df.columns:\n",
    "        axes[0,0].plot(df['epoch'], df['train/box_loss'], label='Train Box Loss', alpha=0.7)\n",
    "    if 'val/box_loss' in df.columns:\n",
    "        axes[0,0].plot(df['epoch'], df['val/box_loss'], label='Val Box Loss', alpha=0.7)\n",
    "    axes[0,0].set_title('Box Loss')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # mAP metrics\n",
    "    if 'metrics/mAP50(B)' in df.columns:\n",
    "        axes[0,1].plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP@0.5', alpha=0.7)\n",
    "    if 'metrics/mAP50-95(B)' in df.columns:\n",
    "        axes[0,1].plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP@0.5:0.95', alpha=0.7)\n",
    "    axes[0,1].set_title('mAP Metrics')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('mAP')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision and Recall\n",
    "    if 'metrics/precision(B)' in df.columns:\n",
    "        axes[1,0].plot(df['epoch'], df['metrics/precision(B)'], label='Precision', alpha=0.7)\n",
    "    if 'metrics/recall(B)' in df.columns:\n",
    "        axes[1,0].plot(df['epoch'], df['metrics/recall(B)'], label='Recall', alpha=0.7)\n",
    "    axes[1,0].set_title('Precision & Recall')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Score')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Learning rate\n",
    "    lr_cols = [col for col in df.columns if 'lr' in col.lower()]\n",
    "    if lr_cols:\n",
    "        for lr_col in lr_cols[:2]:  # Show max 2 LR schedules\n",
    "            axes[1,1].plot(df['epoch'], df[lr_col], label=lr_col, alpha=0.7)\n",
    "        axes[1,1].set_title('Learning Rate')\n",
    "        axes[1,1].set_xlabel('Epoch')\n",
    "        axes[1,1].set_ylabel('Learning Rate')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save plots\n",
    "    plots_dir = RESULTS_DIR / \"plots\"\n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(plots_dir / f\"{experiment_name}_training_curves.png\", dpi=300, bbox_inches='tight')\n",
    "    print(f\"📈 Training plots saved to: {plots_dir / f'{experiment_name}_training_curves.png'}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Analyze results if training was completed\nif 'results' in locals() and results:\n    experiment_path = EXPERIMENTS_DIR / experiment_name\n    training_df = analyze_training_results(experiment_path)\nelse:\n    print(\"⚠️  No training results to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 9. Find and Copy Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_copy_best_model(experiment_dir, final_models_dir):\n",
    "    \"\"\"Find best model weights and copy to final models directory\"\"\"\n",
    "    \n",
    "    # Look for best.pt in experiment directory\n",
    "    best_model_path = experiment_dir / \"weights\" / \"best.pt\"\n",
    "    \n",
    "    if not best_model_path.exists():\n",
    "        print(f\"❌ Best model not found at: {best_model_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Copy to final models directory with descriptive name\n",
    "    final_model_name = f\"yolov8n_indonesian_plates_{experiment_name}.pt\"\n",
    "    final_model_path = final_models_dir / final_model_name\n",
    "    \n",
    "    import shutil\n",
    "    shutil.copy2(best_model_path, final_model_path)\n",
    "    \n",
    "    # Also create a symbolic link to \"best_model.pt\" as specified in CLAUDE.md\n",
    "    best_model_link = final_models_dir / \"best_model.pt\"\n",
    "    if best_model_link.exists():\n",
    "        best_model_link.unlink()\n",
    "    \n",
    "    try:\n",
    "        # Try to create symbolic link (may not work on all systems)\n",
    "        best_model_link.symlink_to(final_model_name)\n",
    "        print(f\"🔗 Created symbolic link: {best_model_link}\")\n",
    "    except OSError:\n",
    "        # Fallback: copy the file\n",
    "        shutil.copy2(final_model_path, best_model_link)\n",
    "        print(f\"📄 Created copy: {best_model_link}\")\n",
    "    \n",
    "    print(f\"✅ Best model saved to: {final_model_path}\")\n",
    "    \n",
    "    # Get model file size\n",
    "    model_size_mb = final_model_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"📏 Model size: {model_size_mb:.1f} MB\")\n",
    "    \n",
    "    # Check if it meets size requirement (< 50MB from CLAUDE.md)\n",
    "    if model_size_mb < 50:\n",
    "        print(\"✅ Model size meets requirement (< 50MB)\")\n",
    "    else:\n",
    "        print(f\"⚠️  Model size exceeds 50MB requirement ({model_size_mb:.1f} MB)\")\n",
    "    \n",
    "    return final_model_path\n",
    "\n",
    "# Find and copy best model\nif 'results' in locals() and results:\n    experiment_path = EXPERIMENTS_DIR / experiment_name\n    best_model_path = find_and_copy_best_model(experiment_path, FINAL_MODELS_DIR)\nelse:\n    print(\"⚠️  No trained model to copy\")\n    best_model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 10. Model Performance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_performance(model_path, test_data_path):\n",
    "    \"\"\"Test model performance on inference speed and accuracy\"\"\"\n",
    "    \n",
    "    if not model_path or not model_path.exists():\n",
    "        print(\"❌ Model not available for testing\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"🧪 Testing model performance: {model_path.name}\")\n",
    "    \n",
    "    # Load trained model\n",
    "    trained_model = YOLO(str(model_path))\n",
    "    \n",
    "    # Get test images\n",
    "    test_images_dir = test_data_path / \"test\" / \"images\"\n",
    "    if not test_images_dir.exists():\n",
    "        # Fallback to validation images\n",
    "        test_images_dir = test_data_path / \"val\" / \"images\"\n",
    "    \n",
    "    if not test_images_dir.exists():\n",
    "        print(\"❌ No test images found\")\n",
    "        return None\n",
    "    \n",
    "    test_images = list(test_images_dir.glob(\"*.jpg\")) + list(test_images_dir.glob(\"*.png\"))\n",
    "    \n",
    "    if not test_images:\n",
    "        print(\"❌ No test images found\")\n",
    "        return None\n",
    "    \n",
    "    # Test on sample images\n",
    "    sample_images = test_images[:10]  # Test on 10 images\n",
    "    print(f\"Testing on {len(sample_images)} sample images...\")\n",
    "    \n",
    "    total_time = 0\n",
    "    detections_count = 0\n",
    "    \n",
    "    for img_path in sample_images:\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Run inference\n",
    "        results = trained_model(str(img_path), conf=training_config['confidence_threshold'])\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        inference_time = (end_time - start_time).total_seconds() * 1000  # ms\n",
    "        \n",
    "        total_time += inference_time\n",
    "        \n",
    "        # Count detections\n",
    "        if results and len(results) > 0:\n",
    "            detections_count += len(results[0].boxes) if results[0].boxes is not None else 0\n",
    "    \n",
    "    # Calculate average performance\n",
    "    avg_inference_time = total_time / len(sample_images)\n",
    "    avg_detections = detections_count / len(sample_images)\n",
    "    \n",
    "    print(f\"\\n⚡ Performance Results:\")\n",
    "    print(f\"Average inference time: {avg_inference_time:.1f} ms\")\n",
    "    print(f\"Average detections per image: {avg_detections:.1f}\")\n",
    "    \n",
    "    # Check if meets speed requirement (< 100ms from CLAUDE.md)\n",
    "    speed_target = 100  # ms\n",
    "    if avg_inference_time < speed_target:\n",
    "        print(f\"✅ Speed requirement met (< {speed_target}ms)\")\n",
    "    else:\n",
    "        print(f\"⚠️  Speed requirement not met ({avg_inference_time:.1f}ms > {speed_target}ms)\")\n",
    "    \n",
    "    # Test with confidence threshold\n",
    "    print(f\"\\n🎯 Testing with confidence threshold: {training_config['confidence_threshold']}\")\n",
    "    \n",
    "    performance_results = {\n",
    "        \"avg_inference_time_ms\": avg_inference_time,\n",
    "        \"avg_detections_per_image\": avg_detections,\n",
    "        \"confidence_threshold\": training_config['confidence_threshold'],\n",
    "        \"meets_speed_requirement\": avg_inference_time < speed_target,\n",
    "        \"sample_size\": len(sample_images)\n",
    "    }\n",
    "    \n",
    "    return performance_results\n",
    "\n",
    "# Test model performance\nif best_model_path and DATASET_PATH.exists():\n    performance_results = test_model_performance(best_model_path, DATASET_PATH)\nelse:\n    print(\"⚠️  Cannot test model performance - model or dataset not available\")\n    performance_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 11. Generate Training Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_report():\n",
    "    \"\"\"Generate comprehensive training report\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"training_config\": training_config,\n",
    "        \"model_info\": {\n",
    "            \"architecture\": \"YOLOv8n\",\n",
    "            \"pretrained\": True,\n",
    "            \"framework\": \"ultralytics\",\n",
    "            \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        },\n",
    "        \"dataset_info\": dataset_config if dataset_config else {},\n",
    "        \"performance_results\": performance_results if performance_results else {},\n",
    "        \"model_paths\": {\n",
    "            \"best_model\": str(best_model_path) if best_model_path else None,\n",
    "            \"experiment_dir\": str(EXPERIMENTS_DIR / experiment_name)\n",
    "        },\n",
    "        \"requirements_check\": {}\n",
    "    }\n",
    "    \n",
    "    # Check requirements from CLAUDE.md\n",
    "    requirements = {\n",
    "        \"target_map50\": training_config['target_map50'],\n",
    "        \"speed_target_ms\": 100,\n",
    "        \"model_size_target_mb\": 50,\n",
    "        \"confidence_threshold\": training_config['confidence_threshold']\n",
    "    }\n",
    "    \n",
    "    # Check final metrics if available\n",
    "    if 'training_df' in locals() and training_df is not None:\n",
    "        final_metrics = training_df.iloc[-1]\n",
    "        \n",
    "        if 'metrics/mAP50(B)' in training_df.columns:\n",
    "            final_map50 = final_metrics['metrics/mAP50(B)']\n",
    "            report[\"final_metrics\"] = {\n",
    "                \"mAP50\": final_map50,\n",
    "                \"mAP50_95\": final_metrics.get('metrics/mAP50-95(B)', None),\n",
    "                \"precision\": final_metrics.get('metrics/precision(B)', None),\n",
    "                \"recall\": final_metrics.get('metrics/recall(B)', None)\n",
    "            }\n",
    "            \n",
    "            report[\"requirements_check\"][\"map50_achieved\"] = final_map50 >= requirements[\"target_map50\"]\n",
    "    \n",
    "    # Check model size\n",
    "    if best_model_path and best_model_path.exists():\n",
    "        model_size_mb = best_model_path.stat().st_size / (1024 * 1024)\n",
    "        report[\"model_size_mb\"] = model_size_mb\n",
    "        report[\"requirements_check\"][\"size_requirement_met\"] = model_size_mb < requirements[\"model_size_target_mb\"]\n",
    "    \n",
    "    # Check speed requirement\n",
    "    if performance_results:\n",
    "        report[\"requirements_check\"][\"speed_requirement_met\"] = performance_results[\"meets_speed_requirement\"]\n",
    "    \n",
    "    # Calculate overall success\n",
    "    checks = report[\"requirements_check\"]\n",
    "    success_rate = sum(checks.values()) / len(checks) if checks else 0\n",
    "    report[\"overall_success_rate\"] = success_rate\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate comprehensive report\ntraining_report = generate_training_report()\n",
    "\n",
    "# Display report summary\nprint(\"\\n📋 Training Report Summary\")\nprint(\"=\" * 50)\nprint(f\"Generated: {training_report['timestamp']}\")\nprint(f\"Experiment: {training_report['experiment_name']}\")\n\nif \"final_metrics\" in training_report:\n    metrics = training_report[\"final_metrics\"]\n    print(f\"\\n🎯 Final Performance:\")\n    for metric, value in metrics.items():\n        if value is not None:\n            print(f\"  {metric}: {value:.4f}\")\n\nif \"model_size_mb\" in training_report:\n    print(f\"\\n📏 Model Size: {training_report['model_size_mb']:.1f} MB\")\n\nif training_report[\"requirements_check\"]:\n    print(f\"\\n✅ Requirements Check:\")\n    for req, passed in training_report[\"requirements_check\"].items():\n        status = \"✅\" if passed else \"❌\"\n        print(f\"  {status} {req.replace('_', ' ').title()}: {passed}\")\n    \n    print(f\"\\n🏆 Overall Success Rate: {training_report['overall_success_rate']:.1%}\")\n\n# Save detailed report\nreports_dir = RESULTS_DIR / \"reports\"\nreports_dir.mkdir(parents=True, exist_ok=True)\n\nwith open(reports_dir / f\"{experiment_name}_training_report.json\", 'w') as f:\n    json.dump(training_report, f, indent=2, default=str)\n\nprint(f\"\\n📄 Detailed report saved to: {reports_dir / f'{experiment_name}_training_report.json'}\")\n\n# Finalize W&B if used\nif use_wandb:\n    # Log final metrics to W&B\n    if \"final_metrics\" in training_report:\n        wandb.log(training_report[\"final_metrics\"])\n    \n    wandb.finish()\n    print(\"🔬 W&B experiment completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 12. Production Integration Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_production():\n",
    "    \"\"\"Prepare model and instructions for production integration\"\"\"\n",
    "    \n",
    "    print(\"🚀 Preparing for production integration...\")\n",
    "    \n",
    "    # Check if we have a trained model\n",
    "    if not best_model_path or not best_model_path.exists():\n",
    "        print(\"❌ No trained model available for production\")\n",
    "        return False\n",
    "    \n",
    "    # Production integration info from CLAUDE.md\n",
    "    production_info = {\n",
    "        \"model_file\": \"best_model.pt\",\n",
    "        \"target_location\": \"../license-plate/cached_models/yolov8_indonesian_plates.pt\",\n",
    "        \"expected_output_format\": {\n",
    "            \"success\": \"bool\",\n",
    "            \"detections\": \"list of detection objects\",\n",
    "            \"total_detections\": \"int\",\n",
    "            \"total_processing_time_ms\": \"int\",\n",
    "            \"error\": \"str or None\"\n",
    "        },\n",
    "        \"confidence_threshold\": training_config['confidence_threshold'],\n",
    "        \"integration_points\": {\n",
    "            \"model_storage\": \"homepage/utils/model_storage.py\",\n",
    "            \"ml_pipeline\": \"homepage/utils/ml_pipeline.py\",\n",
    "            \"database_models\": \"homepage/models.py\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create integration instructions\n",
    "    instructions = f\"\"\"# Production Integration Instructions\n",
    "\n",
    "## Model Information\n",
    "- **Model**: {best_model_path.name}\n",
    "- **Architecture**: YOLOv8n\n",
    "- **Size**: {training_report.get('model_size_mb', 'Unknown'):.1f} MB\n",
    "- **Target Performance**: mAP@0.5 > {training_config['target_map50']}\n",
    "- **Confidence Threshold**: {training_config['confidence_threshold']}\n",
    "\n",
    "## Integration Steps\n",
    "\n",
    "1. **Copy Model to Production**:\n",
    "   ```bash\n",
    "   cp {best_model_path} ../license-plate/cached_models/yolov8_indonesian_plates.pt\n",
    "   ```\n",
    "\n",
    "2. **Update Model Path** (if needed):\n",
    "   - File: `homepage/utils/model_storage.py`\n",
    "   - Expected path: `cached_models/yolov8_indonesian_plates.pt`\n",
    "\n",
    "3. **Test Integration**:\n",
    "   ```bash\n",
    "   cd ../license-plate\n",
    "   python manage.py shell -c \"\n",
    "   from homepage.utils.ml_pipeline import detect_license_plate\n",
    "   result = detect_license_plate('path/to/test/image.jpg')\n",
    "   print(result)\n",
    "   \"\n",
    "   ```\n",
    "\n",
    "## Expected Output Format\n",
    "```json\n",
    "{\n",
    "  \"success\": true,\n",
    "  \"detections\": [\n",
    "    {\n",
    "      \"license_plate_number\": \"L 1234 AB\",\n",
    "      \"confidence_score\": 0.92,\n",
    "      \"bbox\": [450, 310, 680, 375],\n",
    "      \"processing_time_ms\": 85,\n",
    "      \"detection_index\": 0\n",
    "    }\n",
    "  ],\n",
    "  \"total_detections\": 1,\n",
    "  \"total_processing_time_ms\": 95,\n",
    "  \"error\": null\n",
    "}\n",
    "```\n",
    "\n",
    "## Performance Targets Met\n",
    "{chr(10).join([f'- {req.replace(\"_\", \" \").title()}: {\"✅\" if passed else \"❌\"}' for req, passed in training_report.get(\"requirements_check\", {}).items()])}\n",
    "\n",
    "## Next Steps\n",
    "1. Copy model to production environment\n",
    "2. Test integration with existing ml_pipeline.py\n",
    "3. Verify database integration works\n",
    "4. Test with real license plate images\n",
    "5. Monitor performance in production\n",
    "\"\"\"\n",
    "    \n",
    "    # Save integration instructions\n",
    "    instructions_file = FINAL_MODELS_DIR / \"INTEGRATION_INSTRUCTIONS.md\"\n",
    "    with open(instructions_file, 'w') as f:\n",
    "        f.write(instructions)\n",
    "    \n",
    "    print(f\"📄 Integration instructions saved to: {instructions_file}\")\n",
    "    \n",
    "    # Create production info JSON\n",
    "    production_file = FINAL_MODELS_DIR / \"production_info.json\"\n",
    "    with open(production_file, 'w') as f:\n",
    "        json.dump(production_info, f, indent=2)\n",
    "    \n",
    "    print(f\"📄 Production info saved to: {production_file}\")\n",
    "    \n",
    "    # Show quick integration command\n",
    "    print(f\"\\n🚀 Quick Integration Command:\")\n",
    "    print(f\"cp {best_model_path} ../license-plate/cached_models/yolov8_indonesian_plates.pt\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Prepare for production\nproduction_ready = prepare_for_production()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook completed the YOLOv8 model training for Indonesian license plate detection:\n",
    "\n",
    "### ✅ Completed Tasks:\n",
    "- Pre-trained YOLOv8n model loaded and configured\n",
    "- Training with CLAUDE.md specifications (epochs=100, patience=20, batch=16)\n",
    "- Weights & Biases experiment tracking (optional)\n",
    "- Training progress monitoring and metrics visualization\n",
    "- Best model selection and export\n",
    "- Performance testing (speed and accuracy)\n",
    "- Comprehensive training report generation\n",
    "- Production integration preparation\n",
    "\n",
    "### 🎯 Training Configuration Used:\n",
    "- **Model**: YOLOv8n (pre-trained)\n",
    "- **Epochs**: 100 with early stopping (patience=20)\n",
    "- **Batch Size**: 16 (adjusted for GPU memory)\n",
    "- **Image Size**: 640x640\n",
    "- **Optimizer**: AdamW\n",
    "- **Learning Rate**: 0.001\n",
    "\n",
    "### 📊 Performance Targets (CLAUDE.md):\n",
    "- **Accuracy**: mAP@0.5 > 0.85\n",
    "- **Speed**: < 100ms per image (GPU inference)\n",
    "- **Model Size**: < 50MB for deployment\n",
    "- **Confidence Threshold**: >= 0.3\n",
    "\n",
    "### 🚀 Production Integration:\n",
    "- **Model Location**: `models/final/best_model.pt`\n",
    "- **Target Path**: `../license-plate/cached_models/yolov8_indonesian_plates.pt`\n",
    "- **Integration Instructions**: Available in `models/final/INTEGRATION_INSTRUCTIONS.md`\n",
    "\n",
    "### 🎯 Next Steps:\n",
    "1. **If training successful**: Proceed to Notebook 05 (Evaluation & Export)\n",
    "2. **If targets not met**: Consider hyperparameter tuning or additional training\n",
    "3. **Production Integration**: Follow integration instructions to deploy model\n",
    "4. **Testing**: Verify model works with production pipeline\n",
    "\n",
    "The trained model is ready for evaluation and production integration!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}