#!/usr/bin/env python3
import sys
from pathlib import Path
import torch
from ultralytics import YOLO
import time
import numpy as np

print("=== MANUAL MODEL EVALUATION ===")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# Load model
model_path = "models/final/best_model.pt"
print(f"\nüì¶ Loading model: {model_path}")

try:
    model = YOLO(model_path)
    print("‚úÖ Model loaded successfully")
    
    # Model info
    print(f"Model device: {model.device}")
    print(f"Model classes: {model.names}")
    
    # Check model size
    model_size = Path(model_path).stat().st_size / (1024 * 1024)
    print(f"Model size: {model_size:.1f} MB")
    
    # Speed test
    print("\n‚ö° Speed Test (20 iterations):")
    test_image = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
    
    # Warmup
    for _ in range(3):
        _ = model(test_image, verbose=False)
    
    # Measure speed
    times = []
    for i in range(20):
        start = time.time()
        results = model(test_image, verbose=False)
        end = time.time()
        times.append((end - start) * 1000)
    
    avg_time = np.mean(times)
    print(f"Average inference time: {avg_time:.1f} ms")
    print(f"Speed target (<100ms): {'‚úÖ PASS' if avg_time < 100 else '‚ùå FAIL'}")
    print(f"Size target (<50MB): {'‚úÖ PASS' if model_size < 50 else '‚ùå FAIL'}")
    
    # Dataset evaluation if available
    data_yaml = "dataset/plat-kendaraan/data.yaml"
    if Path(data_yaml).exists():
        print(f"\nüìä Running dataset evaluation...")
        try:
            val_results = model.val(data=data_yaml, verbose=False)
            if hasattr(val_results, 'box'):
                box_results = val_results.box
                map50 = float(box_results.map50) if hasattr(box_results, 'map50') else 0.0
                map50_95 = float(box_results.map) if hasattr(box_results, 'map') else 0.0
                precision = float(box_results.mp) if hasattr(box_results, 'mp') else 0.0
                recall = float(box_results.mr) if hasattr(box_results, 'mr') else 0.0
                
                print(f"mAP@0.5: {map50:.3f}")
                print(f"mAP@0.5:0.95: {map50_95:.3f}")
                print(f"Precision: {precision:.3f}")
                print(f"Recall: {recall:.3f}")
                print(f"Performance target (>0.85): {'‚úÖ PASS' if map50 > 0.85 else '‚ùå FAIL'}")
                
                # Summary
                print(f"\nüéØ FINAL SUMMARY:")
                print(f"  Model Size: {model_size:.1f} MB ({'‚úÖ' if model_size < 50 else '‚ùå'})")
                print(f"  Speed: {avg_time:.1f} ms ({'‚úÖ' if avg_time < 100 else '‚ùå'})")
                print(f"  Accuracy: {map50:.3f} ({'‚úÖ' if map50 > 0.85 else '‚ùå'})")
                
                all_pass = model_size < 50 and avg_time < 100 and map50 > 0.85
                print(f"  Overall: {'üéâ PRODUCTION READY!' if all_pass else '‚ö†Ô∏è NEEDS IMPROVEMENT'}")
            else:
                print("‚ùå Could not extract metrics from validation")
        except Exception as e:
            print(f"‚ùå Dataset evaluation failed: {e}")
    else:
        print(f"‚ö†Ô∏è Dataset not found: {data_yaml}")
    
except Exception as e:
    print(f"‚ùå Model loading failed: {e}")

print("\n=== TEST COMPLETED ===")
